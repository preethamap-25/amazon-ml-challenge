{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vipulchinmay/amazon-ml-challenge/blob/main/amazon_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna==3.6.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuNTzpLEKdwz",
        "outputId": "38e5d1e4-376e-459a-8810-bbcc698f656e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna==3.6.1 in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (1.16.5)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna==3.6.1) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna==3.6.1) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.3.0->optuna==3.6.1) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna==3.6.1) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aqF7WYhBbQz2",
        "outputId": "9212af6f-5fad-4e01-98dc-8b18645fc3e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Smart Product Pricing — CNN-based multimodal model\n",
        "# Using images, text, and numeric features to predict price\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Configuration\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/dataset\")\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/output\")\n",
        "IMAGE_DIR = Path(\"/content/drive/MyDrive/dataset/images\")\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "TEXT_SEQ_LEN = 64\n",
        "EMBED_DIM = 64\n",
        "MAX_TOKENS = 20000\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Load Dataset\n",
        "\n",
        "TRAIN_CSV = DATA_DIR / \"train.csv\"\n",
        "TEST_CSV = DATA_DIR / \"test.csv\"\n",
        "\n",
        "train = pd.read_csv(TRAIN_CSV)\n",
        "test = pd.read_csv(TEST_CSV)\n",
        "\n",
        "train['catalog_content'] = train['catalog_content'].astype(str)\n",
        "test['catalog_content'] = test['catalog_content'].astype(str)\n",
        "\n",
        "print(f\"Train shape: {train.shape}\")\n",
        "print(f\"Test shape: {test.shape}\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Feature Engineering\n",
        "\n",
        "IPQ_PAT = re.compile(r'(\\d+\\s*(?:pack|pcs|pieces|count|ct|pk|packets|bottles)?)', flags=re.IGNORECASE)\n",
        "\n",
        "def extract_ipq(text):\n",
        "    if pd.isna(text):\n",
        "        return 1\n",
        "    m = IPQ_PAT.search(text)\n",
        "    if m:\n",
        "        nums = re.findall(r'\\d+', m.group(1))\n",
        "        if nums:\n",
        "            return int(nums[0])\n",
        "    return 1\n",
        "\n",
        "train['ipq'] = train['catalog_content'].apply(extract_ipq)\n",
        "test['ipq'] = test['catalog_content'].apply(extract_ipq)\n",
        "train['text_len'] = train['catalog_content'].str.len()\n",
        "test['text_len'] = test['catalog_content'].str.len()\n",
        "train['price_log1p'] = np.log1p(train['price'].clip(lower=0))\n",
        "\n",
        "print(\"Feature engineering complete\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Text Vectorization\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=MAX_TOKENS, output_sequence_length=TEXT_SEQ_LEN)\n",
        "vectorizer.adapt(train['catalog_content'].values)\n",
        "vocab_size = len(vectorizer.get_vocabulary())\n",
        "print(f'Vocabulary size: {vocab_size}')\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Efficient TF Dataset Generator\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "def load_and_preprocess_image(image_path):\n",
        "    \"\"\"Load and preprocess a single image\"\"\"\n",
        "    # Read the image file\n",
        "    img = tf.io.read_file(image_path)\n",
        "    # Decode it into a dense vector\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    # Resize to the desired size\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    # Normalize to [0, 1]\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "def create_image_path(sample_id):\n",
        "    \"\"\"Create image path from sample_id\"\"\"\n",
        "    return str(IMAGE_DIR / f\"{sample_id}.jpg\")\n",
        "\n",
        "def load_image_with_fallback(image_path):\n",
        "    \"\"\"Load image with fallback to zeros if file doesn't exist\"\"\"\n",
        "    # Check if file exists\n",
        "    file_exists = tf.io.gfile.exists(image_path)\n",
        "\n",
        "    if file_exists:\n",
        "        try:\n",
        "            return load_and_preprocess_image(image_path)\n",
        "        except:\n",
        "            return tf.zeros((*IMG_SIZE, 3), dtype=tf.float32)\n",
        "    else:\n",
        "        return tf.zeros((*IMG_SIZE, 3), dtype=tf.float32)\n",
        "\n",
        "def make_tf_dataset(df, is_train=True):\n",
        "    \"\"\"Create TensorFlow dataset efficiently\"\"\"\n",
        "    print(f\"Creating dataset for {len(df)} samples...\")\n",
        "\n",
        "    # Create image paths\n",
        "    image_paths = [create_image_path(sid) for sid in df['sample_id'].values]\n",
        "\n",
        "    # Vectorize all texts at once\n",
        "    texts_vectorized = vectorizer(df['catalog_content'].values).numpy()\n",
        "\n",
        "    # Get numeric features\n",
        "    numerics = df[['ipq', 'text_len']].values.astype('float32')\n",
        "\n",
        "    if is_train:\n",
        "        targets = df['price_log1p'].values.astype('float32')\n",
        "\n",
        "        # Create dataset from components\n",
        "        def data_generator():\n",
        "            for i in range(len(df)):\n",
        "                img = load_image_with_fallback(image_paths[i])\n",
        "                yield (\n",
        "                    {\n",
        "                        \"image\": img,\n",
        "                        \"text\": texts_vectorized[i],\n",
        "                        \"numeric\": numerics[i]\n",
        "                    },\n",
        "                    targets[i]\n",
        "                )\n",
        "\n",
        "        output_signature = (\n",
        "            {\n",
        "                \"image\": tf.TensorSpec(shape=(*IMG_SIZE, 3), dtype=tf.float32),\n",
        "                \"text\": tf.TensorSpec(shape=(TEXT_SEQ_LEN,), dtype=tf.int64),\n",
        "                \"numeric\": tf.TensorSpec(shape=(2,), dtype=tf.float32)\n",
        "            },\n",
        "            tf.TensorSpec(shape=(), dtype=tf.float32)\n",
        "        )\n",
        "\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            data_generator,\n",
        "            output_signature=output_signature\n",
        "        )\n",
        "        dataset = dataset.shuffle(2048, seed=SEED).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    else:\n",
        "        def data_generator():\n",
        "            for i in range(len(df)):\n",
        "                img = load_image_with_fallback(image_paths[i])\n",
        "                yield {\n",
        "                    \"image\": img,\n",
        "                    \"text\": texts_vectorized[i],\n",
        "                    \"numeric\": numerics[i]\n",
        "                }\n",
        "\n",
        "        output_signature = {\n",
        "            \"image\": tf.TensorSpec(shape=(*IMG_SIZE, 3), dtype=tf.float32),\n",
        "            \"text\": tf.TensorSpec(shape=(TEXT_SEQ_LEN,), dtype=tf.int64),\n",
        "            \"numeric\": tf.TensorSpec(shape=(2,), dtype=tf.float32)\n",
        "        }\n",
        "\n",
        "        dataset = tf.data.Dataset.from_generator(\n",
        "            data_generator,\n",
        "            output_signature=output_signature\n",
        "        )\n",
        "        dataset = dataset.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    print(\"Dataset created successfully\")\n",
        "    return dataset\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Split train/validation\n",
        "\n",
        "print(\"\\nSplitting data...\")\n",
        "train_df, val_df = train_test_split(train, test_size=0.1, random_state=SEED)\n",
        "print(f\"Train samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "print(\"\\nCreating training dataset...\")\n",
        "train_ds = make_tf_dataset(train_df, is_train=True)\n",
        "\n",
        "print(\"\\nCreating validation dataset...\")\n",
        "val_ds = make_tf_dataset(val_df, is_train=True)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Build Multimodal Model\n",
        "\n",
        "print(\"\\nBuilding model...\")\n",
        "\n",
        "# Image branch\n",
        "img_input = keras.Input(shape=(*IMG_SIZE, 3), name=\"image\")\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(img_input)\n",
        "x = layers.MaxPool2D(2)(x)\n",
        "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = layers.MaxPool2D(2)(x)\n",
        "x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "img_out = layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "# Text branch\n",
        "text_input = keras.Input(shape=(TEXT_SEQ_LEN,), dtype='int64', name=\"text\")\n",
        "emb = layers.Embedding(vocab_size, EMBED_DIM)(text_input)\n",
        "t = layers.Conv1D(128, 3, activation='relu', padding='same')(emb)\n",
        "t = layers.GlobalMaxPool1D()(t)\n",
        "text_out = layers.Dense(64, activation='relu')(t)\n",
        "\n",
        "# Numeric branch\n",
        "num_input = keras.Input(shape=(2,), name=\"numeric\")\n",
        "n = layers.Dense(32, activation='relu')(num_input)\n",
        "num_out = layers.Dense(16, activation='relu')(n)\n",
        "\n",
        "# Combine\n",
        "combined = layers.concatenate([img_out, text_out, num_out])\n",
        "combined = layers.Dense(256, activation='relu')(combined)\n",
        "combined = layers.Dropout(0.3)(combined)\n",
        "combined = layers.Dense(64, activation='relu')(combined)\n",
        "out = layers.Dense(1, activation='linear', name=\"price_log1p\")(combined)\n",
        "\n",
        "model = keras.Model(inputs=[img_input, text_input, num_input], outputs=out)\n",
        "model.summary()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Compile Model\n",
        "\n",
        "def smape_tf(y_true, y_pred):\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "    y_true_v = tf.math.expm1(y_true)\n",
        "    y_pred_v = tf.math.expm1(y_pred)\n",
        "    denom = (tf.abs(y_true_v) + tf.abs(y_pred_v)) / 2.0\n",
        "    diff = tf.abs(y_true_v - y_pred_v)\n",
        "    return tf.reduce_mean(tf.where(denom == 0, 0.0, diff / denom)) * 100.0\n",
        "\n",
        "print(\"\\nCompiling model...\")\n",
        "model.compile(optimizer='adam', loss='mae', metrics=[smape_tf])\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Callbacks\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        str(OUTPUT_DIR / 'best_model.h5'),\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        min_lr=1e-6,\n",
        "        verbose=1\n",
        "    ),\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=4,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Train\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting training...\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Evaluate\n",
        "\n",
        "print(\"\\nEvaluating on validation set...\")\n",
        "val_preds_log = model.predict(val_ds, verbose=1)\n",
        "val_trues_log = val_df['price_log1p'].values[:len(val_preds_log)]\n",
        "\n",
        "val_preds = np.expm1(val_preds_log.ravel())\n",
        "val_trues = np.expm1(val_trues_log)\n",
        "\n",
        "mae = mean_absolute_error(val_trues, val_preds)\n",
        "print(f\"\\nValidation MAE: {mae:.4f}\")\n",
        "\n",
        "# Calculate SMAPE manually\n",
        "smape = np.mean(2 * np.abs(val_preds - val_trues) / (np.abs(val_preds) + np.abs(val_trues))) * 100\n",
        "print(f\"Validation SMAPE: {smape:.4f}%\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## Predict on Test Set\n",
        "\n",
        "print(\"\\nCreating test dataset...\")\n",
        "test_ds = make_tf_dataset(test, is_train=False)\n",
        "\n",
        "print(\"\\nGenerating predictions...\")\n",
        "preds_log = model.predict(test_ds, verbose=1)\n",
        "preds = np.expm1(preds_log.ravel())\n",
        "preds = np.clip(preds, 0.01, None)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'sample_id': test['sample_id'].values[:len(preds)],\n",
        "    'price': preds\n",
        "})\n",
        "submission_path = OUTPUT_DIR / 'submission.csv'\n",
        "submission.to_csv(submission_path, index=False)\n",
        "print(f\"\\nSubmission saved to: {submission_path}\")\n",
        "print(f\"Submission shape: {submission.shape}\")\n",
        "print(f\"\\nFirst few predictions:\\n{submission.head()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "592yMWxTJGrb",
        "outputId": "54916afa-5766-4542-ffe2-510ca1d938d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (75000, 4)\n",
            "Test shape: (75000, 3)\n",
            "Feature engineering complete\n",
            "Vocabulary size: 20000\n",
            "\n",
            "Splitting data...\n",
            "Train samples: 67500, Validation samples: 7500\n",
            "\n",
            "Creating training dataset...\n",
            "Creating dataset for 67500 samples...\n",
            "Dataset created successfully\n",
            "\n",
            "Creating validation dataset...\n",
            "Creating dataset for 7500 samples...\n",
            "Dataset created successfully\n",
            "\n",
            "Building model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │  \u001b[38;5;34m1,280,000\u001b[0m │ text[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m24,704\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ numeric             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m96\u001b[0m │ numeric[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m208\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│                     │                   │            │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m53,504\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ price_log1p (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ text[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ numeric             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ numeric[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│                     │                   │            │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">53,504</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ price_log1p (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,493,361\u001b[0m (5.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,493,361</span> (5.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,493,361\u001b[0m (5.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,493,361</span> (5.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compiling model...\n",
            "\n",
            "==================================================\n",
            "Starting training...\n",
            "==================================================\n",
            "\n",
            "Epoch 1/5\n",
            "   2110/Unknown \u001b[1m143s\u001b[0m 62ms/step - loss: 1755135.1250 - smape_tf: nan"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_loss improved from inf to 0.62215, saving model to /content/drive/MyDrive/output/best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 68ms/step - loss: 1754756.6250 - smape_tf: nan - val_loss: 0.6222 - val_smape_tf: 61.2237 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m2107/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.5235 - smape_tf: 52.5343\n",
            "Epoch 2: val_loss did not improve from 0.62215\n",
            "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 65ms/step - loss: 0.5235 - smape_tf: 52.5308 - val_loss: 0.6430 - val_smape_tf: 63.1610 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m2108/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 0.4618 - smape_tf: 46.8401\n",
            "Epoch 3: val_loss did not improve from 0.62215\n",
            "\n",
            "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 66ms/step - loss: 0.4618 - smape_tf: 46.8380 - val_loss: 0.6407 - val_smape_tf: 62.9790 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m2109/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 0.3974 - smape_tf: 40.7355\n",
            "Epoch 4: val_loss did not improve from 0.62215\n",
            "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 67ms/step - loss: 0.3973 - smape_tf: 40.7336 - val_loss: 0.6406 - val_smape_tf: 62.9858 - learning_rate: 5.0000e-04\n",
            "Epoch 5/5\n",
            "\u001b[1m2108/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.3523 - smape_tf: 36.3891\n",
            "Epoch 5: val_loss did not improve from 0.62215\n",
            "\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m2110/2110\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 67ms/step - loss: 0.3523 - smape_tf: 36.3868 - val_loss: 0.6637 - val_smape_tf: 65.0021 - learning_rate: 5.0000e-04\n",
            "Epoch 5: early stopping\n",
            "Restoring model weights from the end of the best epoch: 1.\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Evaluating on validation set...\n",
            "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step\n",
            "\n",
            "Validation MAE: 18.7671\n",
            "Validation SMAPE: 84.2773%\n",
            "\n",
            "Creating test dataset...\n",
            "Creating dataset for 75000 samples...\n",
            "Dataset created successfully\n",
            "\n",
            "Generating predictions...\n",
            "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 41ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Submission saved to: /content/drive/MyDrive/output/submission.csv\n",
            "Submission shape: (75000, 2)\n",
            "\n",
            "First few predictions:\n",
            "   sample_id      price\n",
            "0     100179  10.760299\n",
            "1     245611   9.746073\n",
            "2     146263  15.343919\n",
            "3      95658   6.046479\n",
            "4      36806  17.322092\n"
          ]
        }
      ]
    }
  ]
}