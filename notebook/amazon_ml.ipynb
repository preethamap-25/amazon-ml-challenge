{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "88ea238666db4b85a2817133820b4be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2de9cf7b0e4a4fe98d243f1cfa4c0a60",
              "IPY_MODEL_fdf40aa8caeb4a27a99e55e314ea676f",
              "IPY_MODEL_ed5bb5bf4346485cb715dffe1d9338b0"
            ],
            "layout": "IPY_MODEL_e41668ac808f4f118bc1e0c9f4993063"
          }
        },
        "2de9cf7b0e4a4fe98d243f1cfa4c0a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_072c35e906d34aae8ced11064a68afad",
            "placeholder": "​",
            "style": "IPY_MODEL_a302e67166be4f589407a84015e455ed",
            "value": "100%"
          }
        },
        "fdf40aa8caeb4a27a99e55e314ea676f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c2ca6f37ff844399ebf2f9dd16f1e89",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ab892f5f0a34ad5a1efeb78380acbdb",
            "value": 10000
          }
        },
        "ed5bb5bf4346485cb715dffe1d9338b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6b57d42e82240e79ff6be053383bd87",
            "placeholder": "​",
            "style": "IPY_MODEL_c92f509eb73849cfa595ea35d4fbb97e",
            "value": " 10000/10000 [00:03&lt;00:00, 2810.93it/s]"
          }
        },
        "e41668ac808f4f118bc1e0c9f4993063": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "072c35e906d34aae8ced11064a68afad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a302e67166be4f589407a84015e455ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c2ca6f37ff844399ebf2f9dd16f1e89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab892f5f0a34ad5a1efeb78380acbdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d6b57d42e82240e79ff6be053383bd87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c92f509eb73849cfa595ea35d4fbb97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vipulchinmay/amazon-ml-challenge/blob/main/amazon_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna==3.6.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuNTzpLEKdwz",
        "outputId": "e597abba-cf76-4921-a800-f876a1e30df7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna==3.6.1 in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (1.16.5)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna==3.6.1) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna==3.6.1) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna==3.6.1) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.3.0->optuna==3.6.1) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna==3.6.1) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aqF7WYhBbQz2",
        "outputId": "3474fc0f-b4a2-4370-9b3c-6be318db8f8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# Smart Product Pricing — CNN-based multimodal model\n",
        "# Jupyter-style notebook script (can run in Jupyter or VSCode as a Python script with cells)\n",
        "# Goal: predict product price using catalog text + product image + simple engineered numeric features.\n",
        "# WARNING: This notebook does NOT use any pretrained models, external APIs, or API keys.\n",
        "# It uses custom CNN for images and Conv1D for text. It also computes SMAPE and optional classification metrics (accuracy, F1) by binning prices.\n",
        "\n",
        "# %%\n",
        "# Imports\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "from time import sleep\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, f1_score\n",
        "\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "\n",
        "# %% [markdown]\n",
        "# Configuration\n",
        "# Adjust these paths if running in a different environment. In Kaggle, dataset files live at ../input/... or dataset folder.\n",
        "\n",
        "# If you're running locally, set DATA_DIR to the folder where train.csv/test.csv exist.\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/amazon-ml-challenge/dataset\")\n",
        "OUTPUT_DIR = Path(\"/content/drive/MyDrive/amazon-ml-challenge/output\")\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_CSV = DATA_DIR / \"train.csv\"\n",
        "TEST_CSV = DATA_DIR / \"test.csv\"\n",
        "\n",
        "# Where to store downloaded images\n",
        "IMAGE_DIR = Path(\"/content/drive/MyDrive/amazon-ml-challenge/images\")\n",
        "IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Model / training params\n",
        "IMG_SIZE = (128, 128)        # small to keep training time reasonable\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "MAX_TOKENS = 20000\n",
        "TEXT_SEQ_LEN = 64\n",
        "EMBED_DIM = 64\n",
        "\n",
        "# Whether to preload images to disk. If dataset already contains images locally, set to False.\n",
        "DOWNLOAD_IMAGES = True\n",
        "\n",
        "# %% [markdown]\n",
        "# Utility: robust image downloader\n",
        "\n",
        "# %%\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "\n",
        "def download_image(url, dest_path, timeout=10):\n",
        "    \"\"\"Download image with retries. Returns True on success.\"\"\"\n",
        "    try:\n",
        "        resp = session.get(url, timeout=timeout)\n",
        "        if resp.status_code == 200:\n",
        "            with open(dest_path, 'wb') as f:\n",
        "                f.write(resp.content)\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "# %%\n",
        "if not TRAIN_CSV.exists():\n",
        "    # try kaggle-like path\n",
        "    alt = list(Path('/content/drive/MyDrive/amazon-ml-challenge/dataset').glob('**/train.csv'))\n",
        "    if alt:\n",
        "        TRAIN_CSV = alt[0]\n",
        "\n",
        "if not TEST_CSV.exists():\n",
        "    alt = list(Path('/content/drive/MyDrive/amazon-ml-challenge/dataset').glob('**/test.csv'))\n",
        "    if alt:\n",
        "        TEST_CSV = alt[0]\n",
        "\n",
        "print('Using train:', TRAIN_CSV)\n",
        "print('Using test :', TEST_CSV)\n",
        "\n",
        "train = pd.read_csv(TRAIN_CSV)\n",
        "test = pd.read_csv(TEST_CSV)\n",
        "\n",
        "print('train shape', train.shape, 'test shape', test.shape)\n",
        "\n",
        "# %% [markdown]\n",
        "# Basic preprocessing and feature engineering\n",
        "# - Extract title/description/ipq from catalog_content if possible\n",
        "# - Simple numeric features: length of text, number tokens\n",
        "# - Extract item pack quantity (IPQ) by regex\n",
        "\n",
        "# %%\n",
        "import re\n",
        "\n",
        "IPQ_PAT = re.compile(r'(\\d+\\s*(?:pack|pcs|pieces|count|ct|pk|packets|bottles)?)', flags=re.IGNORECASE)\n",
        "\n",
        "\n",
        "def extract_ipq(text):\n",
        "    if pd.isna(text):\n",
        "        return 1\n",
        "    m = IPQ_PAT.search(text)\n",
        "    if m:\n",
        "        s = m.group(1)\n",
        "        nums = re.findall(r'\\d+', s)\n",
        "        if nums:\n",
        "            return int(nums[0])\n",
        "    return 1\n",
        "\n",
        "train['catalog_content'] = train['catalog_content'].astype(str)\n",
        "test['catalog_content'] = test['catalog_content'].astype(str)\n",
        "\n",
        "train['ipq'] = train['catalog_content'].apply(extract_ipq)\n",
        "test['ipq'] = test['catalog_content'].apply(extract_ipq)\n",
        "\n",
        "train['text_len'] = train['catalog_content'].str.len()\n",
        "test['text_len'] = test['catalog_content'].str.len()\n",
        "\n",
        "# log transform target for stability\n",
        "train['price_log1p'] = np.log1p(train['price'].clip(lower=0))\n",
        "\n",
        "# quick EDA\n",
        "print('price stats:', train['price'].describe())\n",
        "print('ipq unique', train['ipq'].nunique())\n",
        "\n",
        "# %% [markdown]\n",
        "# Download images (optional). This may be slow for 75k images; consider running in Kaggle where images can be downloaded.\n",
        "\n",
        "# %%\n",
        "if DOWNLOAD_IMAGES:\n",
        "    def ensure_images(df, id_col='sample_id'):\n",
        "        missing = 0\n",
        "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            sid = row['sample_id']\n",
        "            url = row.get('image_link', '')\n",
        "            if not isinstance(url, str) or url.strip() == '' or pd.isna(url):\n",
        "                missing += 1\n",
        "                continue\n",
        "            fname = IMAGE_DIR / f\"{sid}.jpg\"\n",
        "            if fname.exists():\n",
        "                continue\n",
        "            ok = download_image(url, fname)\n",
        "            if not ok:\n",
        "                missing += 1\n",
        "        print('done. missing:', missing)\n",
        "\n",
        "    # Only download a subset for faster experimentation. Full training requires all images.\n",
        "    SAMPLE_IMG_LIMIT = 10000  # set to an integer like 5000 to limit\n",
        "    if SAMPLE_IMG_LIMIT is None:\n",
        "        ensure_images(train)\n",
        "    else:\n",
        "        ensure_images(train.head(SAMPLE_IMG_LIMIT))\n",
        "\n",
        "# %% [markdown]\n",
        "# Text vectorization using Keras TextVectorization (no pretrained embeddings)\n",
        "\n",
        "# %%\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=MAX_TOKENS, output_sequence_length=TEXT_SEQ_LEN)\n",
        "\n",
        "# adapt on train text\n",
        "vectorizer.adapt(train['catalog_content'].values)\n",
        "\n",
        "# token count check\n",
        "vocab_size = len(vectorizer.get_vocabulary())\n",
        "print('vocab size', vocab_size)\n",
        "\n",
        "# prepare tokenized arrays\n",
        "train_text = vectorizer(train['catalog_content'].values)\n",
        "test_text = vectorizer(test['catalog_content'].values)\n",
        "\n",
        "# %% [markdown]\n",
        "# Build TF dataset generator: yields (image, text, numeric) -> price\n",
        "\n",
        "# %%\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "\n",
        "def load_image_for_id(sample_id, target_size=IMG_SIZE):\n",
        "    p = IMAGE_DIR / f\"{sample_id}.jpg\"\n",
        "    if not p.exists():\n",
        "        # return blank image\n",
        "        return np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8)\n",
        "    img = tf.io.read_file(str(p))\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, target_size)\n",
        "    img = img / 255.0\n",
        "    return img\n",
        "\n",
        "\n",
        "def make_tf_dataset(df, is_train=True):\n",
        "    sample_ids = df['sample_id'].astype(str).values\n",
        "    texts = df['catalog_content'].astype(str).values\n",
        "    ipq = df['ipq'].values.astype('float32')\n",
        "    text_len = df['text_len'].values.astype('float32')\n",
        "    if is_train:\n",
        "        y = df['price_log1p'].values.astype('float32')\n",
        "\n",
        "    def gen():\n",
        "        for i, sid in enumerate(sample_ids):\n",
        "            img = load_image_for_id(sid)\n",
        "            txt_tok = vectorizer(tf.constant([texts[i]]))[0]\n",
        "            num = np.array([ipq[i], text_len[i]], dtype='float32')\n",
        "            x = {\n",
        "                \"image\": img,\n",
        "                \"text\": txt_tok,\n",
        "                \"numeric\": num\n",
        "            }\n",
        "            if is_train:\n",
        "                yield x, np.array([y[i]], dtype='float32')\n",
        "            else:\n",
        "                yield x\n",
        "\n",
        "    # --- output signature for TF Dataset ---\n",
        "    input_signature = {\n",
        "        \"image\": tf.TensorSpec(shape=(*IMG_SIZE, 3), dtype=tf.float32),\n",
        "        \"text\": tf.TensorSpec(shape=(TEXT_SEQ_LEN,), dtype=tf.int64),\n",
        "        \"numeric\": tf.TensorSpec(shape=(2,), dtype=tf.float32),\n",
        "    }\n",
        "\n",
        "    if is_train:\n",
        "        ds = tf.data.Dataset.from_generator(\n",
        "            gen,\n",
        "            output_signature=(input_signature, tf.TensorSpec(shape=(1,), dtype=tf.float32))\n",
        "        )\n",
        "        ds = ds.shuffle(2048, seed=SEED).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "    else:\n",
        "        ds = tf.data.Dataset.from_generator(\n",
        "            gen,\n",
        "            output_signature=input_signature\n",
        "        ).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "    return ds\n",
        "\n",
        "# %% [markdown]\n",
        "# Model: image CNN + text Conv1D + numeric inputs -> regression\n",
        "\n",
        "# %%\n",
        "# image branch\n",
        "img_input = keras.Input(shape=(*IMG_SIZE, 3), name='image')\n",
        "x = layers.Conv2D(32, 3, activation='relu', padding='same')(img_input)\n",
        "x = layers.MaxPool2D(2)(x)\n",
        "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
        "x = layers.MaxPool2D(2)(x)\n",
        "x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(128, activation='relu')(x)\n",
        "img_out = layers.Dropout(0.2)(x)\n",
        "\n",
        "# text branch\n",
        "text_input = keras.Input(shape=(TEXT_SEQ_LEN,), dtype='int64', name='text')\n",
        "emb = layers.Embedding(input_dim=vocab_size, output_dim=EMBED_DIM, mask_zero=False)(text_input)\n",
        "# Conv1D blocks\n",
        "t = layers.Conv1D(128, 3, activation='relu', padding='same')(emb)\n",
        "t = layers.GlobalMaxPool1D()(t)\n",
        "t = layers.Dense(64, activation='relu')(t)\n",
        "text_out = layers.Dropout(0.2)(t)\n",
        "\n",
        "# numeric branch\n",
        "num_input = keras.Input(shape=(2,), name='numeric')\n",
        "n = layers.Dense(32, activation='relu')(num_input)\n",
        "num_out = layers.Dense(16, activation='relu')(n)\n",
        "\n",
        "# combine\n",
        "combined = layers.concatenate([img_out, text_out, num_out])\n",
        "combined = layers.Dense(256, activation='relu')(combined)\n",
        "combined = layers.Dropout(0.3)(combined)\n",
        "combined = layers.Dense(64, activation='relu')(combined)\n",
        "# final output predicts log1p(price); use linear activation and then expm1 when decoding\n",
        "out = layers.Dense(1, activation='linear', name='price_log1p')(combined)\n",
        "\n",
        "model = keras.Model(inputs=[img_input, text_input, num_input], outputs=out)\n",
        "model.summary()\n",
        "\n",
        "# %% [markdown]\n",
        "# Custom SMAPE metric and loss\n",
        "\n",
        "#%% [markdown]\n",
        "@tf.function\n",
        "def smape_tf(y_true, y_pred):\n",
        "    # Always ensure proper shape\n",
        "    y_true = tf.reshape(y_true, [-1])\n",
        "    y_pred = tf.reshape(y_pred, [-1])\n",
        "\n",
        "    # Convert from log1p scale back to original\n",
        "    y_true_v = tf.math.expm1(y_true)\n",
        "    y_pred_v = tf.math.expm1(y_pred)\n",
        "\n",
        "    denom = (tf.abs(y_true_v) + tf.abs(y_pred_v)) / 2.0\n",
        "    diff = tf.abs(y_true_v - y_pred_v)\n",
        "\n",
        "    sm = tf.where(denom == 0, 0.0, diff / denom)\n",
        "    return tf.reduce_mean(sm) * 100.0\n",
        "\n",
        "\n",
        "# Use MAE on log1p as loss (stable) and track SMAPE metric\n",
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss='mae', metrics=[smape_tf])\n",
        "\n",
        "# %% [markdown]\n",
        "# Train/validation split and datasets\n",
        "\n",
        "# %%\n",
        "train_df, val_df = train_test_split(train, test_size=0.1, random_state=SEED)\n",
        "print(train_df.shape, val_df.shape)\n",
        "\n",
        "train_ds = make_tf_dataset(train_df, is_train=True)\n",
        "val_ds = make_tf_dataset(val_df, is_train=True)\n",
        "\n",
        "# %% [markdown]\n",
        "# Callbacks\n",
        "\n",
        "# %%\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(OUTPUT_DIR / 'best_model.h5', save_best_only=True, monitor='val_loss'),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# %% [markdown]\n",
        "# Fit\n",
        "\n",
        "\n",
        "# %%\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\n",
        "\n",
        "# %% [markdown]\n",
        "# Evaluate on validation set and compute SMAPE/MAE in original price scale\n",
        "\n",
        "# %%\n",
        "# Predict on validation set\n",
        "val_preds_log = model.predict(val_ds)\n",
        "# collect true values from val_df\n",
        "val_trues_log = val_df['price_log1p'].values[:len(val_preds_log)]\n",
        "\n",
        "val_preds = np.expm1(val_preds_log.ravel())\n",
        "val_trues = np.expm1(val_trues_log)\n",
        "\n",
        "mae = mean_absolute_error(val_trues, val_preds)\n",
        "print('Validation MAE:', mae)\n",
        "\n",
        "# SMAPE calculation\n",
        "\n",
        "def smape_numpy(a, f):\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2.0\n",
        "    diff = np.abs(a - f)\n",
        "    sm = np.where(denom == 0, 0.0, diff / denom)\n",
        "    return np.mean(sm) * 100.0\n",
        "\n",
        "print('Validation SMAPE:', smape_numpy(val_trues, val_preds))\n",
        "\n",
        "# Optional: compute classification metrics by binning prices into quantiles\n",
        "bins = np.quantile(train['price'].clip(lower=0), [0.0, 0.33, 0.66, 1.0])\n",
        "val_true_bins = np.digitize(val_trues, bins) - 1\n",
        "val_pred_bins = np.digitize(val_preds, bins) - 1\n",
        "print('Validation accuracy (binned):', accuracy_score(val_true_bins, val_pred_bins))\n",
        "print('Validation F1 (macro) (binned):', f1_score(val_true_bins, val_pred_bins, average='macro'))\n",
        "\n",
        "# %% [markdown]\n",
        "# Predict on test set and prepare submission\n",
        "\n",
        "# %%\n",
        "# make test dataset\n",
        "\n",
        "test_ds = make_tf_dataset(test, is_train=False)\n",
        "\n",
        "preds_log = model.predict(test_ds)\n",
        "preds = np.expm1(preds_log.ravel())\n",
        "# ensure positive\n",
        "preds = np.clip(preds, a_min=0.01, a_max=None)\n",
        "\n",
        "submission = pd.DataFrame({'sample_id': test['sample_id'].values[:len(preds)], 'price': preds})\n",
        "submission.to_csv(OUTPUT_DIR / 'submission.csv', index=False)\n",
        "print('Saved submission to', OUTPUT_DIR / 'submission.csv')\n",
        "\n",
        "# %% [markdown]\n",
        "# Notes, next steps & tips\n",
        "# - This is a solid baseline multimodal model that uses a small custom CNN (no pretrained weights) and a Conv1D text encoder.\n",
        "# - To improve performance:\n",
        "#   * Train longer, use larger images (224x224) and deeper CNN architectures (but still from scratch).\n",
        "#   * Use data augmentations for images (random crop, flip) and text augmentations (synonym swap).\n",
        "#   * Use K-Fold cross-validation and ensemble multiple models.\n",
        "#   * Increase vocab size, embedding dimension and text sequence length for richer text modelling.\n",
        "#   * Consider using transformer-style encoders **trained from scratch** if you are allowed larger compute.\n",
        "# - Remember F1/accuracy are only meaningful if we convert regression to classification by binning. For fair evaluation on Kaggle leaderboard use SMAPE.\n",
        "\n",
        "# End of notebook\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "88ea238666db4b85a2817133820b4be2",
            "2de9cf7b0e4a4fe98d243f1cfa4c0a60",
            "fdf40aa8caeb4a27a99e55e314ea676f",
            "ed5bb5bf4346485cb715dffe1d9338b0",
            "e41668ac808f4f118bc1e0c9f4993063",
            "072c35e906d34aae8ced11064a68afad",
            "a302e67166be4f589407a84015e455ed",
            "9c2ca6f37ff844399ebf2f9dd16f1e89",
            "6ab892f5f0a34ad5a1efeb78380acbdb",
            "d6b57d42e82240e79ff6be053383bd87",
            "c92f509eb73849cfa595ea35d4fbb97e"
          ]
        },
        "id": "8ANE5GEIWZTv",
        "outputId": "5e679cb2-2cca-4e42-d11c-6654c0ce3961"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using train: /content/drive/MyDrive/amazon-ml-challenge/dataset/train.csv\n",
            "Using test : /content/drive/MyDrive/amazon-ml-challenge/dataset/test.csv\n",
            "train shape (75000, 4) test shape (75000, 3)\n",
            "price stats: count    75000.000000\n",
            "mean        23.647654\n",
            "std         33.376932\n",
            "min          0.130000\n",
            "25%          6.795000\n",
            "50%         14.000000\n",
            "75%         28.625000\n",
            "max       2796.000000\n",
            "Name: price, dtype: float64\n",
            "ipq unique 940\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "88ea238666db4b85a2817133820b4be2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done. missing: 0\n",
            "vocab size 20000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m896\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_16    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d_16… │\n",
              "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_17    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_8         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │  \u001b[38;5;34m1,280,000\u001b[0m │ text[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_17… │\n",
              "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │     \u001b[38;5;34m24,704\u001b[0m │ embedding_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv2d_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ numeric             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m16,512\u001b[0m │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m8,256\u001b[0m │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │         \u001b[38;5;34m96\u001b[0m │ numeric[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_24          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_47[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_25          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_48[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ dense_49[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_8       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m208\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_24[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_25[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
              "│                     │                   │            │ dense_50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │     \u001b[38;5;34m53,504\u001b[0m │ concatenate_8[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_26          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_51[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m16,448\u001b[0m │ dropout_26[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ price_log1p (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m65\u001b[0m │ dense_52[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_16    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d_16… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ text (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_17    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ embedding_8         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ text[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_17… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │ embedding_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ numeric             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ global_average_p… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ global_max_pooli… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> │ numeric[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_24          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_47[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_25          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_48[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ dense_49[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_8       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_24[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_25[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
              "│                     │                   │            │ dense_50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">53,504</span> │ concatenate_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout_26          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_51[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │ dropout_26[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ price_log1p (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │ dense_52[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,493,361\u001b[0m (5.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,493,361</span> (5.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,493,361\u001b[0m (5.70 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,493,361</span> (5.70 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(67500, 7) (7500, 7)\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: `generator` yielded an element of shape (1,) where an element of shape () was expected.\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (1,) where an element of shape () was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  TypeError: `generator` yielded an element of shape (1,) where an element of shape () was expected.\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (1,) where an element of shape () was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_990127]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-455834810.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;31m# %% [markdown]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  TypeError: `generator` yielded an element of shape (1,) where an element of shape () was expected.\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (1,) where an element of shape () was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  TypeError: `generator` yielded an element of shape (1,) where an element of shape () was expected.\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.12/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 235, in generator_py_func\n    raise TypeError(\n\nTypeError: `generator` yielded an element of shape (1,) where an element of shape () was expected.\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_multi_step_on_iterator_990127]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# 1️⃣ Simulated example data\n",
        "# -----------------------------\n",
        "num_samples = 1000\n",
        "\n",
        "images = np.random.rand(num_samples, 128, 128, 3).astype(\"float32\")\n",
        "texts = np.random.randint(0, 20000, (num_samples, 64)).astype(\"int32\")\n",
        "numerics = np.random.rand(num_samples, 2).astype(\"float32\")\n",
        "targets = np.random.rand(num_samples).astype(\"float32\")  # regression\n",
        "\n",
        "# -----------------------------\n",
        "# 2️⃣ Create a tf.data.Dataset\n",
        "# -----------------------------\n",
        "ds = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        \"image\": images,\n",
        "        \"text\": texts,\n",
        "        \"numeric\": numerics,\n",
        "    },\n",
        "    targets\n",
        "))\n",
        "\n",
        "# -----------------------------\n",
        "# 3️⃣ Split into train/val\n",
        "# -----------------------------\n",
        "train_size = int(0.8 * num_samples)\n",
        "val_size = num_samples - train_size\n",
        "\n",
        "train_ds = ds.take(train_size)\n",
        "val_ds = ds.skip(train_size)\n",
        "\n",
        "# -----------------------------\n",
        "# 4️⃣ Optimize pipeline performance\n",
        "# -----------------------------\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = (\n",
        "    train_ds\n",
        "    .shuffle(buffer_size=train_size)\n",
        "    .batch(32)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    val_ds\n",
        "    .batch(32)\n",
        "    .prefetch(AUTOTUNE)\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 5️⃣ Define the multimodal model\n",
        "# -----------------------------\n",
        "# Image branch\n",
        "image_input = tf.keras.Input(shape=(128, 128, 3), name=\"image\")\n",
        "x_img = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\")(image_input)\n",
        "x_img = tf.keras.layers.MaxPooling2D()(x_img)\n",
        "x_img = tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\")(x_img)\n",
        "x_img = tf.keras.layers.GlobalAveragePooling2D()(x_img)\n",
        "\n",
        "# Text branch\n",
        "text_input = tf.keras.Input(shape=(64,), dtype=tf.int32, name=\"text\")\n",
        "x_txt = tf.keras.layers.Embedding(input_dim=20000, output_dim=128)(text_input)\n",
        "x_txt = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(x_txt)\n",
        "\n",
        "# Numeric branch\n",
        "num_input = tf.keras.Input(shape=(2,), name=\"numeric\")\n",
        "x_num = tf.keras.layers.Dense(16, activation=\"relu\")(num_input)\n",
        "\n",
        "# Combine branches\n",
        "combined = tf.keras.layers.concatenate([x_img, x_txt, x_num])\n",
        "x = tf.keras.layers.Dense(128, activation=\"relu\")(combined)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "output = tf.keras.layers.Dense(1, activation=\"linear\")(x)\n",
        "\n",
        "model = tf.keras.Model(\n",
        "    inputs={\"image\": image_input, \"text\": text_input, \"numeric\": num_input},\n",
        "    outputs=output\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 6️⃣ Compile and train\n",
        "# -----------------------------\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "    loss=\"mse\",\n",
        "    metrics=[\"mae\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# %% [markdown]\n",
        "# Evaluate on validation set and compute SMAPE/MAE in original price scale\n",
        "\n",
        "# %%\n",
        "# Predict on validation set\n",
        "val_preds_log = model.predict(val_ds)\n",
        "# collect true values from val_df\n",
        "val_trues_log = val_df['price_log1p'].values[:len(val_preds_log)]\n",
        "\n",
        "val_preds = np.expm1(val_preds_log.ravel())\n",
        "val_trues = np.expm1(val_trues_log)\n",
        "\n",
        "mae = mean_absolute_error(val_trues, val_preds)\n",
        "print('Validation MAE:', mae)\n",
        "\n",
        "# SMAPE calculation\n",
        "\n",
        "def smape_numpy(a, f):\n",
        "    denom = (np.abs(a) + np.abs(f)) / 2.0\n",
        "    diff = np.abs(a - f)\n",
        "    sm = np.where(denom == 0, 0.0, diff / denom)\n",
        "    return np.mean(sm) * 100.0\n",
        "\n",
        "print('Validation SMAPE:', smape_numpy(val_trues, val_preds))\n",
        "\n",
        "# Optional: compute classification metrics by binning prices into quantiles\n",
        "bins = np.quantile(train['price'].clip(lower=0), [0.0, 0.33, 0.66, 1.0])\n",
        "val_true_bins = np.digitize(val_trues, bins) - 1\n",
        "val_pred_bins = np.digitize(val_preds, bins) - 1\n",
        "print('Validation accuracy (binned):', accuracy_score(val_true_bins, val_pred_bins))\n",
        "print('Validation F1 (macro) (binned):', f1_score(val_true_bins, val_pred_bins, average='macro'))\n",
        "\n",
        "# %% [markdown]\n",
        "# Predict on test set and prepare submission\n",
        "\n",
        "# %%\n",
        "# make test dataset\n",
        "\n",
        "test_ds = make_tf_dataset(test, is_train=False)\n",
        "\n",
        "preds_log = model.predict(test_ds)\n",
        "preds = np.expm1(preds_log.ravel())\n",
        "# ensure positive\n",
        "preds = np.clip(preds, a_min=0.01, a_max=None)\n",
        "\n",
        "submission = pd.DataFrame({'sample_id': test['sample_id'].values[:len(preds)], 'price': preds})\n",
        "submission.to_csv(OUTPUT_DIR / 'submission.csv', index=False)\n",
        "print('Saved submission to', OUTPUT_DIR / 'submission.csv')\n",
        "\n",
        "# %% [markdown]\n",
        "# Notes, next steps & tips\n",
        "# - This is a solid baseline multimodal model that uses a small custom CNN (no pretrained weights) and a Conv1D text encoder.\n",
        "# - To improve performance:\n",
        "#   * Train longer, use larger images (224x224) and deeper CNN architectures (but still from scratch).\n",
        "#   * Use data augmentations for images (random crop, flip) and text augmentations (synonym swap).\n",
        "#   * Use K-Fold cross-validation and ensemble multiple models.\n",
        "#   * Increase vocab size, embedding dimension and text sequence length for richer text modelling.\n",
        "#   * Consider using transformer-style encoders **trained from scratch** if you are allowed larger compute.\n",
        "# - Remember F1/accuracy are only meaningful if we convert regression to classification by binning. For fair evaluation on Kaggle leaderboard use SMAPE.\n",
        "\n",
        "# End of notebook"
      ],
      "metadata": {
        "id": "Cyaly5cqt0m5",
        "outputId": "3ab946b9-5e48-4ba3-924f-147d60e25c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 67ms/step - loss: 0.2743 - mae: 0.4374 - val_loss: 0.0965 - val_mae: 0.2589\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0997 - mae: 0.2651 - val_loss: 0.0866 - val_mae: 0.2489\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.1011 - mae: 0.2732 - val_loss: 0.0860 - val_mae: 0.2468\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0932 - mae: 0.2646 - val_loss: 0.0856 - val_mae: 0.2461\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0872 - mae: 0.2482 - val_loss: 0.0852 - val_mae: 0.2459\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0756 - mae: 0.2338 - val_loss: 0.0854 - val_mae: 0.2466\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0569 - mae: 0.1978 - val_loss: 0.0944 - val_mae: 0.2571\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0186 - mae: 0.1076 - val_loss: 0.0880 - val_mae: 0.2492\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0141 - mae: 0.0929 - val_loss: 0.0900 - val_mae: 0.2518\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0097 - mae: 0.0755 - val_loss: 0.0883 - val_mae: 0.2493\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Validation MAE: 23.837722217094896\n",
            "Validation SMAPE: 175.24012946363268\n",
            "Validation accuracy (binned): 0.3\n",
            "Validation F1 (macro) (binned): 0.15384615384615385\n",
            "\u001b[1m2344/2344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 272ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission to /content/drive/MyDrive/amazon-ml-challenge/output/submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1381119316.py:139: RuntimeWarning: overflow encountered in expm1\n",
            "  preds = np.expm1(preds_log.ravel())\n"
          ]
        }
      ]
    }
  ]
}