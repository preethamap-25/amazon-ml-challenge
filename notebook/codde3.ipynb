{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmeaAKLtqH+vZI049xdZgT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishit009/amazon-ml-challenge/blob/main/codde3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üì¶ IMPORTS\n",
        "# ==================================================\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.applications import ResNet50, resnet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from tqdm import tqdm\n",
        "import joblib"
      ],
      "metadata": {
        "id": "Xdg3QL4pHPNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üìÅ CONFIGURATION\n",
        "# ==================================================\n",
        "TRAIN_CSV = \"path to train.csv\"   # path to your training CSV\n",
        "TEST_CSV = \"path to test.csv\"     # path to your test CSV\n",
        "DOWNLOAD_DIR_TRAIN = \"/content/drive/MyDrive/food_images_train\"\n",
        "DOWNLOAD_DIR_TEST = \"/content/drive/MyDrive/food_images_test\"\n",
        "IMG_SIZE = (224, 224)\n",
        "KFOLDS = 3\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "os.makedirs(DOWNLOAD_DIR_TRAIN, exist_ok=True)\n",
        "os.makedirs(DOWNLOAD_DIR_TEST, exist_ok=True)"
      ],
      "metadata": {
        "id": "CNEJuK7BHP_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üñºÔ∏è IMAGE DOWNLOAD FUNCTION\n",
        "# ==================================================\n",
        "def download_image(url, save_path):\n",
        "    try:\n",
        "        if not os.path.exists(save_path):\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "                img.save(save_path, \"JPEG\", quality=90)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed: {url} ‚Äî {e}\")"
      ],
      "metadata": {
        "id": "yKpzIY2gHUP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üßæ LOAD TRAINING DATA\n",
        "# ==================================================\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "train_df.columns = [c.strip().lower() for c in train_df.columns]\n",
        "train_df[\"filepath\"] = train_df[\"sample_id\"].apply(lambda x: os.path.join(DOWNLOAD_DIR_TRAIN, f\"{x}.jpg\"))\n",
        "\n",
        "# üì• Download missing training images\n",
        "print(\"üì• Checking and downloading missing training images...\")\n",
        "for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    if not os.path.exists(row[\"filepath\"]):\n",
        "        download_image(row[\"image_link\"], row[\"filepath\"])\n",
        "\n",
        "train_df = train_df[train_df[\"filepath\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "print(f\"‚úÖ {len(train_df)} training images ready.\")"
      ],
      "metadata": {
        "id": "2saiP3BVHWeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üî¢ SCALE PRICE VALUES\n",
        "# ==================================================\n",
        "scaler = MinMaxScaler()\n",
        "train_df[\"price_scaled\"] = scaler.fit_transform(train_df[[\"price\"]])\n",
        "joblib.dump(scaler, \"price_scaler.pkl\")"
      ],
      "metadata": {
        "id": "kuuSnXvZHZN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üì∏ DATA GENERATORS\n",
        "# ==================================================\n",
        "datagen_train = ImageDataGenerator(\n",
        "    preprocessing_function=resnet50.preprocess_input,\n",
        "    rotation_range=25,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "datagen_val = ImageDataGenerator(preprocessing_function=resnet50.preprocess_input)"
      ],
      "metadata": {
        "id": "dCa-c1nRHbUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üß† MODEL BUILDER FUNCTION\n",
        "# ==================================================\n",
        "def build_model():\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(64, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "                  loss='mse', metrics=['mae'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "tk9yl5vYHdRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîÑ K-FOLD CROSS VALIDATION\n",
        "# ==================================================\n",
        "kf = KFold(n_splits=KFOLDS, shuffle=True, random_state=42)\n",
        "fold = 1\n",
        "fold_scores = []\n",
        "\n",
        "for train_idx, val_idx in kf.split(train_df):\n",
        "    print(f\"\\nüåÄ Training Fold {fold}/{KFOLDS}\")\n",
        "    train_data = train_df.iloc[train_idx]\n",
        "    val_data = train_df.iloc[val_idx]\n",
        "\n",
        "    train_gen = datagen_train.flow_from_dataframe(\n",
        "        dataframe=train_data,\n",
        "        x_col=\"filepath\",\n",
        "        y_col=\"price_scaled\",\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode=\"raw\"\n",
        "    )\n",
        "\n",
        "    val_gen = datagen_val.flow_from_dataframe(\n",
        "        dataframe=val_data,\n",
        "        x_col=\"filepath\",\n",
        "        y_col=\"price_scaled\",\n",
        "        target_size=IMG_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode=\"raw\",\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    model = build_model()\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=4, min_lr=1e-6),\n",
        "        ModelCheckpoint(f\"best_model_fold{fold}.h5\", save_best_only=True, monitor='val_loss')\n",
        "    ]\n",
        "\n",
        "    # Phase 1 ‚Äî Train with frozen base\n",
        "    model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Phase 2 ‚Äî Fine-tuning last 30 layers\n",
        "    model.layers[0].trainable = True\n",
        "    for layer in model.layers[0].layers[:-30]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                  loss='mse', metrics=['mae'])\n",
        "\n",
        "    model.fit(\n",
        "        train_gen,\n",
        "        validation_data=val_gen,\n",
        "        epochs=10,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    val_loss, val_mae = model.evaluate(val_gen)\n",
        "    fold_scores.append((val_loss, val_mae))\n",
        "    print(f\"‚úÖ Fold {fold} MAE: {val_mae:.4f}\")\n",
        "    fold += 1"
      ],
      "metadata": {
        "id": "em4EhllpHgYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üìà FINAL RESULTS\n",
        "# ==================================================\n",
        "avg_loss = np.mean([l for l, _ in fold_scores])\n",
        "avg_mae  = np.mean([m for _, m in fold_scores])\n",
        "print(f\"\\nüéØ Average Validation Loss: {avg_loss:.4f}, MAE: {avg_mae:.4f}\")\n",
        "print(\"‚úÖ Training completed successfully!\")"
      ],
      "metadata": {
        "id": "HDDc3YU1HiRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXeW-qdTHLB8"
      },
      "outputs": [],
      "source": [
        "# ==================================================\n",
        "# üß™ TESTING ‚Äî PRICE PREDICTION ON TEST DATASET\n",
        "# ==================================================\n",
        "print(\"\\nüß™ Loading test dataset...\")\n",
        "test_df = pd.read_csv(TEST_CSV)\n",
        "test_df.columns = [c.strip().lower() for c in test_df.columns]\n",
        "test_df[\"filepath\"] = test_df[\"sample_id\"].apply(lambda x: os.path.join(DOWNLOAD_DIR_TEST, f\"{x}.jpg\"))\n",
        "\n",
        "# üì• Download test images\n",
        "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "    if not os.path.exists(row[\"filepath\"]):\n",
        "        download_image(row[\"image_link\"], row[\"filepath\"])\n",
        "\n",
        "test_df = test_df[test_df[\"filepath\"].apply(os.path.exists)].reset_index(drop=True)\n",
        "print(f\"‚úÖ {len(test_df)} test images ready for prediction.\")\n",
        "\n",
        "# Use best model (from last fold)\n",
        "best_model_path = f\"best_model_fold{KFOLDS}.h5\"\n",
        "model = tf.keras.models.load_model(best_model_path)\n",
        "\n",
        "# Preprocess test images and predict\n",
        "X_test = np.array([\n",
        "    resnet50.preprocess_input(img_to_array(load_img(p, target_size=IMG_SIZE)))\n",
        "    for p in tqdm(test_df[\"filepath\"], desc=\"üßÆ Preprocessing test images\")\n",
        "])\n",
        "\n",
        "y_pred_scaled = model.predict(X_test)\n",
        "scaler = joblib.load(\"price_scaler.pkl\")\n",
        "y_pred = scaler.inverse_transform(y_pred_scaled)\n",
        "\n",
        "# Save predictions\n",
        "test_df[\"predicted_price\"] = y_pred\n",
        "test_df[[\"sample_id\", \"predicted_price\"]].to_csv(\"predicted_prices.csv\", index=False)\n",
        "print(\"üíæ Predictions saved to predicted_prices.csv\")\n"
      ]
    }
  ]
}